{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160d4d6b-f01c-4678-852a-8bb02dfa7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from time import time, ctime, localtime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split,train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold,  KFold\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248a8436-8241-4ef9-a72b-3737d1e96351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train dataset is : 68\n",
      "number of test dataset is : 64\n"
     ]
    }
   ],
   "source": [
    "# path = \"D:/SleepDataHallym/\"\n",
    "path = \"/home/nyh/SleepDataSets/\"\n",
    "\n",
    "train_list = glob(path + \"train/*\")\n",
    "print(f\"number of train dataset is : {train_list.__len__()}\")\n",
    "\n",
    "test_list = glob(path + \"test/*\")\n",
    "print(f\"number of test dataset is : {test_list.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c587de-827d-452f-b0c7-95ee9531aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cae2630-da4d-41cb-a5f8-14d5bc762d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "n = 10\n",
    "arr = list(range(0,10))\n",
    "result = []\n",
    "for i in range(0, n):\n",
    "    comb = combinations(arr, i)\n",
    "    for element in [*comb]:\n",
    "        if element.__len__() != 0 : result.append(element)\n",
    "         # element.__len__() != 1\n",
    "# print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe60035-cc0a-476e-b4e1-f7c953af2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train dataset is : 68\n",
      "number of test dataset is : 64\n"
     ]
    }
   ],
   "source": [
    "# path = \"D:/SleepDataHallym/\"\n",
    "path = \"/home/nyh/SleepDataSets/\"\n",
    "\n",
    "train_list = glob(path + \"train/*\")\n",
    "print(f\"number of train dataset is : {train_list.__len__()}\")\n",
    "\n",
    "test_list = glob(path + \"test/*\")\n",
    "print(f\"number of test dataset is : {test_list.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fda533-32df-4d6f-bd86-f8c696f40622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainDataset(Dataset):\n",
    "    def read_dataset(self):\n",
    "        all_signals_files = []\n",
    "        all_labels = []\n",
    "\n",
    "        for dataset_folder in self.patient_path:\n",
    "            \n",
    "            # print(dataset_folder)\n",
    "            \n",
    "            signals_path = dataset_folder + \"/\"\n",
    "            signals_list = os.listdir(signals_path)\n",
    "            # print(signals_list)\n",
    "            signals_list.sort()\n",
    "            for signals_filename in signals_list:\n",
    "                signals_file = signals_path+signals_filename \n",
    "                all_signals_files.append(signals_file)\n",
    "                all_labels.append(int(signals_filename.split('.npy')[0].split('_')[-1]))\n",
    "                \n",
    "        return all_signals_files, all_labels, len(all_signals_files)\n",
    "    \n",
    "    \n",
    "    def __init__(self, mode, use_rnn, use_channel = [0,1], transform=None, target_transform=None):\n",
    "        # self.path = \"D:/SleepDataHallym/\"\n",
    "        self.path = \"/home/nyh/SleepDataSets/\"\n",
    "        \n",
    "        if mode == \"train\": \n",
    "            self.patient_path = glob(self.path + \"train/*\")\n",
    "            self.patient_list,self.labels,self.length = self.read_dataset()\n",
    "        elif mode == \"test\": \n",
    "            self.patient_path = glob(self.path + \"test/*\")\n",
    "            self.patient_list,self.labels,self.length = self.read_dataset()\n",
    "        else: return -1\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.signal = []\n",
    "        self.annotation = []\n",
    "        self.use_rnn = use_rnn\n",
    "        self.use_channel = use_channel\n",
    "        \n",
    "    def __len__(self):\n",
    "        # print(len(self.patient_list))\n",
    "        return len(self.patient_list)\n",
    "    \n",
    "    def print_line(cnt, folder_name):\n",
    "        print(f\"{cnt}th file name is {folder_name[-6:]} patient collect\", end= \"\\t\\t\")\n",
    "\n",
    "        if cnt%3 == 0:\n",
    "            print()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        if self.use_rnn == True:\n",
    "            folder_name = '/'.join(self.patient_list[index].split('/')[:-1]) + '/'\n",
    "            folder_list = os.listdir(folder_name)\n",
    "            folder_list.sort()\n",
    "            folder_length = len(folder_list)-1\n",
    "            \n",
    "            # train/patient_num_OSA/current_epoch\n",
    "            current_epoch = int(self.patient_list[index].split('/')[-1].split('_')[0]) \n",
    "            start_index, end_index = current_epoch - 2, current_epoch + 3             \n",
    "\n",
    "            labels = int(folder_list[current_epoch].split('.npy')[0].split('_')[-1])\n",
    "            \n",
    "            signals = None\n",
    "            input_signals = None\n",
    "            \n",
    "            count = 0\n",
    "            for i in range(start_index,end_index):\n",
    "                if i <= 0:\n",
    "                    c_signals = np.load(folder_name+folder_list[0])\n",
    "                    c_signals = c_signals[self.use_channel,:]             \n",
    "                    input_signals = torch.Tensor(c_signals)\n",
    "\n",
    "                elif i >= folder_length:\n",
    "                    c_signals = np.load(folder_name+folder_list[folder_length])\n",
    "                    c_signals = c_signals[self.use_channel,:]\n",
    "                    input_signals = torch.Tensor(c_signals)\n",
    "                else:\n",
    "                    c_signals = np.load(folder_name+folder_list[i])\n",
    "                    c_signals = c_signals[self.use_channel,:]\n",
    "                    input_signals = torch.Tensor(c_signals)\n",
    "                    \n",
    "                input_signals = input_signals.reshape(1, self.use_channel.__len__(), 6000)\n",
    "                \n",
    "                if count == 0:\n",
    "                    signals = input_signals\n",
    "\n",
    "                else:\n",
    "                    # print(signals, type(signals))\n",
    "                    signals = torch.cat((signals, input_signals), 0)\n",
    "\n",
    "                count += 1\n",
    "                        \n",
    "                # print(signals.shape, labels)\n",
    "                # print(input_signals.shape)\n",
    "\n",
    "            return signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6977037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler # 실습 때 사용하였던 SubsetRandomSampler 사용\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_list = CustomTrainDataset(mode = \"train\", use_rnn = True,  use_channel = [2,3,4])\n",
    "test_list = CustomTrainDataset(mode = \"test\", use_rnn = True,  use_channel = [2,3,4])\n",
    "\n",
    "train_size = 0.8 # train size 0.9 , validation size = 0.1\n",
    "num_train = train_list.__len__() # train_data의 크기를 저장 \n",
    "indices = list(range(num_train)) # train_Data 크기에 맞는 인덱스 생성 \n",
    "np.random.shuffle(indices)   #  인덱스를 섞어야 라벨과 데이터가 안섞임\n",
    "split = int(np.floor(train_size * num_train)) # 학습 데이터의 비율과 총 데이터의 크기를 곱한다음 np.floor를 통해 버림을 합니다.\n",
    "train_idx, valid_idx = indices[:split], indices[split:] # 이제 split 된 크기만큼 train과 validation 데이터 셋을 나눠 줍니다. \n",
    " # 이제 나눈 인덱스의 번호로 SubsetSampler를 통해 넣어줍니다\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a902b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49265"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b1b02a-935f-49a0-b4df-747298386006",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_list,batch_size=64, sampler= train_sampler, num_workers = 2)\n",
    "validloader = DataLoader(dataset=train_list,batch_size=64, sampler= valid_sampler, num_workers = 2)\n",
    "testloader = DataLoader(dataset=test_list,batch_size=64, shuffle=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0841a555-e33a-4ee0-b890-0e614c541e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DeepSleepNet(nn.Module):\n",
    "    def __init__(self, in_channel = 3, small_Fs = 8, big_Fs = 6, Fs = 200):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available(): self.use_gpu = True\n",
    "        else: self.use_gpu = False\n",
    "        self.num_directions = 2 #  bidirctionalLSTM \n",
    "        self.num_layers = 1\n",
    "        self.hidden_dim = 512\n",
    "        \n",
    "\n",
    "\n",
    "        self.smallCNN = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channel, out_channels = 64 , kernel_size = Fs//2 , stride = Fs//16, padding = Fs//2//2, bias = False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.MaxPool1d(kernel_size = 8, stride = 8,  padding = 4),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv1d(in_channels = 64 , out_channels = 128 , kernel_size = small_Fs, \n",
    "                        stride = 1, padding= small_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv1d(in_channels = 128 , out_channels = 128 , kernel_size = small_Fs, \n",
    "                        stride = 1, padding = small_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv1d(in_channels = 128 , out_channels = 128 , kernel_size = small_Fs, \n",
    "                        stride = 1, padding= small_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.MaxPool1d(kernel_size = 4, stride = 4,  padding = 2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.bigCNN = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channel, out_channels = 64 , kernel_size = Fs*4 , stride = Fs//2, padding = Fs*4//2, bias = False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.MaxPool1d(kernel_size = 4, stride = 4,  padding = 2), # padding = filter_size//2\n",
    "            nn.Dropout(0.5) ,\n",
    "\n",
    "            nn.Conv1d(in_channels = 64 , out_channels = 128 , kernel_size = big_Fs, stride = 1, padding= big_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv1d(in_channels = 128 , out_channels = 128 , kernel_size = big_Fs, stride = 1, padding= big_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv1d(in_channels = 128 , out_channels = 128 , kernel_size = big_Fs, stride = 1, padding= big_Fs//2, bias = False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.MaxPool1d(kernel_size = 2, stride = 2,  padding = 1)\n",
    "        )\n",
    "\n",
    "        self.lstm_layer1 = nn.LSTM(1280 + 2176, 512, bidirectional = True)\n",
    "        self.lstm_layer2 = nn.LSTM(512 * 2, 512, bidirectional = True)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Linear(1280 + 2176, 1024)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 6)  # big and small conv concat     \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "    def init_hidden1(self, order_signal):\n",
    "        # first = h(hidden) / second = c(cell)\n",
    "        if self.use_gpu:\n",
    "            return (\n",
    "            Variable(torch.zeros(self.num_directions * self.num_layers, order_signal, self.hidden_dim).cuda()),  # hidden\n",
    "            Variable(torch.zeros(self.num_directions * self.num_layers, order_signal, self.hidden_dim).cuda()))  # cell\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_directions * self.num_layers, order_signal, self.hidden_dim)),  # hidden\n",
    "                    Variable(torch.zeros(self.num_directions * self.num_layers, order_signal, self.hidden_dim)))  # cell\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output = None\n",
    "        order_signal = x.shape[1]\n",
    "        \n",
    "        for i in range(order_signal):\n",
    "            # print(x.shape)\n",
    "            cnn = x[:, i, :, :]\n",
    "            # print(cnn.shape)\n",
    "            small = self.smallCNN(cnn)\n",
    "            big = self.bigCNN(cnn)\n",
    "\n",
    "            feature_big = torch.flatten(big, 1)\n",
    "            feature_small = torch.flatten(small, 1)  # (batch, channel)\n",
    "\n",
    "            cnn_output = torch.cat((feature_big, feature_small), dim=1)\n",
    "            \n",
    "            if i == 0:\n",
    "                lstm_output = cnn_output\n",
    "            else:\n",
    "                lstm_output = torch.cat((lstm_output, cnn_output), 1)\n",
    "        \n",
    "        \n",
    "        batch_size = lstm_output.size(0)\n",
    "        lstm_output = lstm_output.reshape(batch_size, -1, 1280+2176) # Afer concat, We got [batchsize , 5 * (1280 + 2176)] -> we append 2 startindex and endindex 3\n",
    "\n",
    "        self.hidden1 = self.init_hidden1(order_signal)\n",
    "        output, self.hidden1 = self.lstm_layer1(lstm_output, self.hidden1)\n",
    "\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        lstm_shortcut = self.shortcut(lstm_output)\n",
    "        \n",
    "        lstm_output, _ = self.lstm_layer1(lstm_output)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        lstm_output, _ = self.lstm_layer2(lstm_output)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "\n",
    "        lstm_output = torch.add(lstm_output, lstm_shortcut)\n",
    "        output = self.dropout(lstm_output)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return(output[:, 2, :]) # return middel signal classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3d88a2-1420-46ee-a606-8f57d382aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start Learning =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632abe6e12714e4fa3669190b7ff4917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bbcccedd0444caa4ebf50880b80364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d5738a7ac40fc9ceb0fd158c2ec4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4e7ee8e5e149189d9f773009248b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100.. Train loss: 1.132.. Train acc: 0.620.. val loss: 0.723.. val accuracy: 0.769.. test loss: 0.723.. test accuracy: 0.769.. \n",
      "Validation loss decreased (inf --> 0.722809).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2336e6875bf24a0fa1eda8217eb3abca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:30<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617c64e1138844398e188b7b4d12bcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b317b2aa3a4518a769e41a9104ddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m####### Test ######\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# validation 과정 no_grad()를 통해 Gradient 계산 안함.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m# validloder에 넣어둔 값 들고오기\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mfor\u001b[39;00m lists \u001b[39min\u001b[39;00m tqdm(testloader, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231302e3131352e3232392e3438222c2275736572223a226e7968222c22706f7274223a313231317d/home/nyh/SleepDataSets/Multi_SleepNet_Plus_RNN_CH234.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         \u001b[39m# 위에서 train 후 validation 실행. train한번 당 validation 1번 실행.\u001b[39;00m\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1174\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1175\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1012\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/home/eslab/anaconda3/envs/nyh/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sched import scheduler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "epochs = 100 # 반복 수는 100\n",
    "batch_size= 64\n",
    "cnt = 0      # early stopping을 적용하기 위해 만들어놓은 cnt\n",
    "\n",
    "# train 및 val Loss 저장 \n",
    "train_loss = torch.zeros(epochs)\n",
    "val_loss = torch.zeros(epochs)\n",
    "test_loss = torch.zeros(epochs)\n",
    "\n",
    "# train 및 val Accuracy 저장 \n",
    "train_acc = torch.zeros(epochs)\n",
    "val_acc = torch.zeros(epochs)\n",
    "test_acc = torch.zeros(epochs)\n",
    "\n",
    "# 초기 Loss값은 무한대\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "# epochs 수만큼 학습 진행\n",
    "print(\"===== Start Learning =====\")\n",
    "\n",
    "model = DeepSleepNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1: # GPU가 2개 이상이라면\n",
    "    model = nn.DataParallel(model) # nn.DataParallel을 사용하여 GPU 병렬 처리 진행\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas = [0.9, 0.999]) # 기본적인 Adam 사용 후 lr 및 Optimizier 변경 예정\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(trainloader, leave=True):    \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # optimizer 초기화 -> 모든 gradient를 초기화 시켜줌으로써 이전에 사용했던 기울기에 더해지지않고 새로 구하게 됨\n",
    "        logits = model.forward(inputs) # validation set인 input값을 넣어서 실행       \n",
    "        loss = criterion(logits, labels) # criterion은 이전에 정의한 CrossEntropy를 통해 Loss 계산\n",
    "        loss.backward() # backward를 통해 역전파 실행 계산된 loss를 가지고 모델의 파라미터 개선\n",
    "        optimizer.step() # optimizer.step()을 통해 개선된 파라미터 적용\n",
    "\n",
    "        train_loss[epoch] += loss.item() # 에포크당 train_loss 누적\n",
    "\n",
    "        ps = F.softmax(logits, dim=1) # softmax함수를 통한 정규화 (0 ~ 1) 사이의 확률로 만들어줌\n",
    "        top_p, top_class = ps.topk(1, dim=1) # topk를 통해 가장 높은 한개를 뽑음\n",
    "\n",
    "        equals = top_class == labels.reshape(top_class.shape)   # 일치하는지 확인.\n",
    "        train_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item()  # 정확도 계산을 위해 float로 타입 변환 후 mean 계산.\n",
    "\n",
    "    # Loss의 평균을 구하기\n",
    "    train_loss[epoch] /= len(trainloader)\n",
    "    train_acc[epoch] /= len(trainloader)\n",
    "    \n",
    "    model.eval()   #dropout Layer와 BatchNormLayer는 eval과정에서 필요하지 않기 때문\n",
    "    with torch.no_grad():  # validation 과정 no_grad()를 통해 Gradient 계산 안함.\n",
    "        # validloder에 넣어둔 값 들고오기\n",
    "\n",
    "        for lists in tqdm(validloader, leave=True):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # 위에서 train 후 validation 실행. train한번 당 validation 1번 실행.\n",
    "\n",
    "            logits = model.forward(inputs) # validation set인 input값을 넣어서 실행 \n",
    "            batch_loss = criterion(logits, labels).detach() # validation set의 Loss 계산\n",
    "            val_loss[epoch] += batch_loss.item() # Loss 값 누적\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = F.softmax(logits, dim=1) # 확률값 구하기 (0~1) 사이로 정규화가 됨\n",
    "            top_p, top_class = ps.topk(1, dim=1) # 가장 높은값 하나를 고르는데 그 값과 idx가져옴\n",
    "            equals = top_class == labels.view(*top_class.shape) # 일치하는지 확인 \n",
    "            val_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item() # # 정확도 계산을 위해 float로 타입 변환 후 mean 계산.\n",
    "\n",
    "        # validation Loss 및 accuracy 평균냄\n",
    "        val_loss[epoch] /= len(validloader)\n",
    "        val_acc[epoch] /= len(validloader)\n",
    "    \n",
    "    ####### Test ######\n",
    "    with torch.no_grad():  # validation 과정 no_grad()를 통해 Gradient 계산 안함.\n",
    "        # validloder에 넣어둔 값 들고오기\n",
    "\n",
    "        for lists in tqdm(testloader, leave=True):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # 위에서 train 후 validation 실행. train한번 당 validation 1번 실행.\n",
    "\n",
    "\n",
    "            logits = model.forward(inputs) # validation set인 input값을 넣어서 실행 \n",
    "            batch_loss = criterion(logits, labels).detach() # validation set의 Loss 계산\\\n",
    "            test_loss[epoch] += batch_loss.item() # Loss 값 누적\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = F.softmax(logits, dim=1) # 확률값 구하기 (0~1) 사이로 정규화가 됨\n",
    "            top_p, top_class = ps.topk(1, dim=1) # 가장 높은값 하나를 고르는데 그 값과 idx가져옴\n",
    "            equals = top_class == labels.view(*top_class.shape) # 일치하는지 확인 \n",
    "            test_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item() # # 정확도 계산을 위해 float로 타입 변환 후 mean 계산.\n",
    "\n",
    "        # validation Loss 및 accuracy 평균냄\n",
    "        test_loss[epoch] /= len(testloader)\n",
    "        test_acc[epoch] /= len(testloader)\n",
    "\n",
    "    ##################### PRINT LOSS & ACC #####################\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {train_loss[epoch]:.3f}.. \"\n",
    "          f\"Train acc: {train_acc[epoch]:.3f}.. \"\n",
    "          f\"val loss: {val_loss[epoch]:.3f}.. \"\n",
    "          f\"val accuracy: {val_acc[epoch]:.3f}.. \"\n",
    "          f\"test loss: {test_loss[epoch]:.3f}.. \"\n",
    "          f\"test accuracy: {test_acc[epoch]:.3f}.. \"\n",
    "         )\n",
    "\n",
    "    ##################### 최적의 모델 저장 #####################\n",
    "    if val_loss[epoch] <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        val_loss[epoch]))\n",
    "        torch.save(model.module.state_dict(), 'model_best_channel_234.pt')\n",
    "        valid_loss_min = val_loss[epoch]\n",
    "\n",
    "        # 가장 낮은 Loss값을 가지게 된다면 Early Stopping count 초기화\n",
    "        cnt = 0\n",
    "\n",
    "    # 20번 이상 Loss 개선이 안된다면 종료\n",
    "    ############# Early Stopping #############\n",
    "    if cnt >= 10:\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "\n",
    "    cnt+=1 #Loss 개선 실패\n",
    "    ########################################################\n",
    "    scheduler.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523b3af-e732-4c40-89fc-e31e1e33f96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07dbf3-545a-42b9-91fd-370bb716fc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nyh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6294b4ef0e4c52c7a2802844b3dd361f2afc0517f08f729f87f8b6d38982a606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
